{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment parameters\n",
      "=====================\n",
      "        exp_dir = .\n",
      "output_filename = C01s002_0_1decay0.1.json\n",
      "\n",
      "\n",
      "Start time is:  2020-10-13 22:02:37.487234\n",
      "[[array([1.000e-06, 1.491e-01]), array([0.3979, 0.5048]), array([0.5653, 0.6822]), array([0.7865, 0.9282]), array([1.975 , 2.0896]), array([3.5805, 3.6872]), array([4.3395, 4.4411]), array([4.5531, 4.688 ]), array([4.7015, 4.7947]), array([5.053 , 5.1868]), array([5.4039, 5.4781])]]\n",
      "\n",
      "Number of jumps: 242\n",
      "\n",
      "per node: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "INFERENCE\n",
      "=========\n",
      "\n",
      "Run VI (vi_exp)\n",
      "------\n",
      "vi random seed: 6891\n",
      "M is: 11\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1342) must match the size of tensor b (0) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-534e6c3e24f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muserlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mout_put_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'C01'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'decay'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecay_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mrun\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_line\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_line\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_line\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_line\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_put_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecay_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-534e6c3e24f4>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(user, start_line, end_line, exp_dir, output_filename, decay_rate, stdout, stderr)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0mintegral_instesity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;31m#coeffs_var, adj_var, mu_var, nu, varsigma = learn_vi(events, vi_seed, inference_param_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mcoeffs_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_vi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvi_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_param_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0;31m#模型参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0madj_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-534e6c3e24f4>\u001b[0m in \u001b[0;36mlearn_vi\u001b[0;34m(events, vi_seed, inference_param_dict, return_learner)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mevents_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevents_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# cast to tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_learner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/20201012/var-hawkes-master-oral-oral/varhawkes/learners.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, events, x0, seed, callback)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# Gradient update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/20201012/var-hawkes-master-oral-oral/varhawkes/models.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(self, x, seed)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Compute a Monte Carlo estimate of the expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objective_l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/20201012/var-hawkes-master-oral-oral/varhawkes/models.py\u001b[0m in \u001b[0;36m_objective_l\u001b[0;34m(self, eps_l, alpha, beta)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meps_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;31m# Compute the importance weights (and their gradients)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mlog_w_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_importance_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Temper the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mlog_w_arr\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/20201012/var-hawkes-master-oral-oral/varhawkes/models.py\u001b[0m in \u001b[0;36m_log_importance_weight\u001b[0;34m(self, eps, alpha, beta)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \"\"\"\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Reparametrize the variational parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Split and reshape parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/20201012/var-hawkes-master-oral-oral/varhawkes/posteriors.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(self, eps, alpha, beta)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mReparamaterization\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mapproximate\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnormal\u001b[0m \u001b[0mposterior\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \"\"\"\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1342) must match the size of tensor b (0) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "from scipy.optimize import fsolve\n",
    "from scipy import log\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "\n",
    "# External libraries\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "# Internal libraries\n",
    "\n",
    "import models\n",
    "import posteriors\n",
    "import priors\n",
    "import hawkes_model, excitation_kernels\n",
    "import learners\n",
    "import utils\n",
    "\n",
    "from make_data_for_samples import make_data              #多个样本数据\n",
    "from make_data_for_estimate import make_estimate_data    #单个样本数据\n",
    "\n",
    "def make_object(module, name, args):\n",
    "    return getattr(module, name)(**args)\n",
    "\n",
    "#def learn_vi(events, end_time, vi_seed, adjacency_true, inference_param_dict, return_learner=False):\n",
    "def learn_vi(events, vi_seed, inference_param_dict, return_learner=False):\n",
    "    # Extract some parameters for easier access\n",
    "    #n_nodes = len(events)\n",
    "    n_events = len(events)\n",
    "    n_nodes = len(events[0])\n",
    "    M = inference_param_dict['excitation']['args'].get('M', 1)\n",
    "    print('M is:',M)\n",
    "    n_params = n_nodes * (n_nodes * M + 1)\n",
    "    n_edges = M * n_nodes ** 2\n",
    "    # Set seed\n",
    "    np.random.seed(vi_seed)\n",
    "    # Set starting pointM * n_nodes ** 2\n",
    "    x0 = torch.tensor(\n",
    "        np.hstack((\n",
    "            np.hstack((  # alpha, the mean of the parameters\n",
    "                np.random.normal(loc=0.1, scale=0.1, size=n_nodes),\n",
    "                np.random.normal(loc=0.1, scale=0.1, size=n_edges),)),\n",
    "            np.hstack((  # beta=log(sigma), log of the variance of the parameters\n",
    "                np.log(np.clip(np.random.normal(loc=0.2, scale=0.1, size=n_nodes), 1e-1, 2.0)),\n",
    "                np.log(np.clip(np.random.normal(loc=0.2, scale=0.1, size=n_edges), 1e-1, 2.0)),))\n",
    "        )),\n",
    "        dtype=torch.float64, requires_grad=True\n",
    "    )\n",
    "    # Init Hawkes process model object\n",
    "    excitation_obj = make_object(excitation_kernels, **inference_param_dict['excitation'])\n",
    "    hawkes_model_obj = hawkes_model.HawkesModel(excitation=excitation_obj, verbose=False)\n",
    "    # Init the posterior object\n",
    "    posterior_obj = make_object(posteriors, **inference_param_dict['posterior'])\n",
    "    # Init the prior object\n",
    "    prior_type = inference_param_dict['prior']['name']\n",
    "    prior_args = inference_param_dict['prior']['args']\n",
    "    prior_args['C'] = torch.tensor(prior_args['C'], dtype=torch.float64)  # cast to tensor\n",
    "    prior_obj = make_object(priors, prior_type, prior_args)\n",
    "    # Init the variational inference model object\n",
    "    model = models.ModelHawkesVariational(\n",
    "        model=hawkes_model_obj, posterior=posterior_obj, prior=prior_obj,\n",
    "        **inference_param_dict['model']['args'])\n",
    "   \n",
    "    # Init the optimizer\n",
    "    opt_type = inference_param_dict['optimizer']['name']\n",
    "    opt_args = inference_param_dict['optimizer']['args']\n",
    "    opt = getattr(optim, opt_type)([x0], **opt_args)\n",
    "    # Init learner\n",
    "    learner = learners.VariationalInferenceLearner(\n",
    "        model=model, optimizer=opt, **inference_param_dict['learner']['args'])\n",
    "    # Fit the model\n",
    "    events_t = [torch.tensor(events_i) for events_i in events]  # cast to tensor\n",
    "    learner.fit(events_t, x0=x0, callback=None)\n",
    "    print()\n",
    "    if return_learner:\n",
    "        return learner\n",
    "    # Extract the mode of the posterior\n",
    "    z_est_mode = learner.model.posterior.mode(learner.coeffs[:n_params], learner.coeffs[n_params:])\n",
    "    adj_est_ora = z_est_mode[n_nodes:].detach()\n",
    "    mu_est_ora = z_est_mode[:n_nodes].detach()\n",
    "    adj_est_ora = adj_est_ora.view(n_nodes, n_nodes, M)\n",
    "    adj_est = z_est_mode[n_nodes:].detach().numpy()\n",
    "    adj_est = np.reshape(adj_est, (n_nodes, n_nodes, M)).sum(-1).ravel()\n",
    "    mu_est = z_est_mode[:n_nodes].detach().numpy()\n",
    "    #mu_est = np.reshape(mu_est,n_nodes).ravel()\n",
    "    coeffs_est = learner.coeffs.detach().numpy()\n",
    "    log_like_sum = hawkes_model_obj.log_likelihood(mu_est_ora,adj_est_ora)\n",
    "    \n",
    "    return coeffs_est, adj_est,mu_est\n",
    "\n",
    "\n",
    "def run(user,start_line,end_line,exp_dir, output_filename, decay_rate=0.01, stdout=None, stderr=None):\n",
    "    # Reset random seed\n",
    "    np.random.seed(None)\n",
    "\n",
    "    if stdout is not None:\n",
    "        sys.stdout = open(stdout, 'w')\n",
    "    if stderr is not None:\n",
    "        sys.stderr = open(stderr, 'w')\n",
    "\n",
    "    print('\\nExperiment parameters')\n",
    "    print('=====================')\n",
    "    print(f'        exp_dir = {exp_dir:s}')\n",
    "    print(f'output_filename = {output_filename:s}')\n",
    "    print(flush=True)\n",
    "    print('\\nStart time is: ', datetime.datetime.today())\n",
    "\n",
    "    result_dict = {}\n",
    "    \n",
    "    data_fileName = \"./data/DSL-StrongPasswordData.xls\"\n",
    "    global events\n",
    "    #events = make_data('s036',12350,12400,data_fileName)\n",
    "    event = make_data(user,start_line,end_line,data_fileName)\n",
    "    \n",
    "    '''\n",
    "    ######一次只训练一个样本，复制多个\n",
    "    times = 20\n",
    "    \n",
    "    #print (event)\n",
    "    events = []\n",
    "    \n",
    "    for i in range(times):\n",
    "        events.extend(event)\n",
    "    print(events)\n",
    "    ######\n",
    "    '''\n",
    "    \n",
    "    n_jumps_per_dim = list(map(len, event[0]))\n",
    "    print(event)\n",
    "    print('\\nNumber of jumps:', len(event[0])*sum(n_jumps_per_dim))\n",
    "    print('\\nper node:', n_jumps_per_dim)\n",
    "    \n",
    "    #11+121\n",
    "    #C_list = [1.0]*132\n",
    "    #11+121*M\n",
    "    M = 11\n",
    "    C_list = [0.1]*(11+121*M)\n",
    "    \n",
    "    #LaplacePrior\n",
    "    '''\n",
    "    param_dict={'inference':{'vi_exp':{'excitation': {'name': 'ExponentialKernel','args': {'decay': 0.01, 'cut_off': 1000.0}}, \n",
    "                          'posterior': {'name': 'LogNormalPosterior', 'args': {}},\n",
    "                          'prior': {'name': 'LaplacePrior', 'args': {'dim': 11, 'n_params': 132, 'C': C_list}}, \n",
    "                          'model': {'args': {'n_samples': 1, 'n_weights': 1, 'weight_temp': 1.0}}, \n",
    "                          'optimizer': {'name': 'Adam', 'args': {'lr': 0.01}}, \n",
    "                          'learner': {'args': {'tol': 1e-04, 'lr_gamma': 0.9999, 'max_iter': 20000, 'hyperparam_momentum': 0.5, 'hyperparam_interval': 100, 'hyperparam_offset': 0}}}}\n",
    "               }\n",
    "    \n",
    "    param_dict['inference']['vi_exp']['excitation']['args']['decay'] = decay_rate\n",
    "    print (param_dict)\n",
    "    '''\n",
    "    \n",
    "\n",
    "    param_dict={'inference':{'vi_exp':{'excitation': {'name': 'MixtureGaussianFilter','args': {'M': 11, \"end_time\": 36, 'cut_off': 1000.0}}, \n",
    "                          'posterior': {'name': 'LogNormalPosterior', 'args': {}},\n",
    "                          'prior': {'name': 'GaussianLaplacianPrior', 'args': {'dim': 11, 'n_params': 1342, 'C': C_list}}, \n",
    "                          'model': {'args': {'n_samples': 1, 'n_weights': 1, 'weight_temp': 1.0}}, \n",
    "                          'optimizer': {'name': 'Adam', 'args': {'lr': 0.01}}, \n",
    "                          'learner': {'args': {'tol': 1e-04, 'lr_gamma': 0.9999, 'max_iter': 20000, 'hyperparam_momentum': 0.5, 'hyperparam_interval': 100, 'hyperparam_offset': 0}}}}\n",
    "               }\n",
    "    #param_dict['inference']['vi_exp']['excitation']['args']['decay'] = decay_rate\n",
    "    #print (param_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    print('\\nINFERENCE')\n",
    "    print('=========')\n",
    "\n",
    "    for key, inference_param_dict in param_dict['inference'].items():\n",
    "        if key.startswith('vi'):\n",
    "            print(f'\\nRun VI ({key:s})')\n",
    "            print('------')\n",
    "            # Set random seed (for reproducibility)\n",
    "            np.random.seed()  # Reset random number generator to avoid dependency on simulation seed\n",
    "            #vi_seed = np.random.randint(2**32 - 1)\n",
    "            vi_seed = np.random.randint(2**16 - 1)\n",
    "            print(f'vi random seed: {vi_seed}')\n",
    "            # Run inference\n",
    "            #global intens_sum\n",
    "            global integral_instesity\n",
    "            #coeffs_var, adj_var, mu_var, nu, varsigma = learn_vi(events, vi_seed, inference_param_dict)\n",
    "            coeffs_var, adj_var, mu_var = learn_vi(event[0], vi_seed, inference_param_dict)\n",
    "            #模型参数\n",
    "            adj_var = adj_var.ravel()\n",
    "            mu_var = mu_var.ravel()            \n",
    "          \n",
    "          \n",
    "            result_dict.update({\n",
    "                key: {\n",
    "                    'vi_seed': vi_seed,             # VI random seed\n",
    "                    'coeffs': coeffs_var.tolist(),  # VI parameters\n",
    "                    'adjacency': adj_var.tolist(),  # VI Estimator\n",
    "                    'mu':  mu_var.tolist(),\n",
    "                }\n",
    "            })\n",
    " \n",
    "\n",
    "    #print('\\n\\nSave results...')\n",
    "    \n",
    "    #print('\\ncoeffs:',  coeffs_var.tolist())\n",
    "    #print( '\\nadjacency:', adj_var.tolist())\n",
    "    #print('\\nmu:', mu_var.tolist())\n",
    "    #print('\\nvarsigma:',varsigma)\n",
    "\n",
    "    with open(os.path.join(exp_dir, output_filename), 'w') as output_file:\n",
    "        json.dump(result_dict, output_file)\n",
    "\n",
    "    # Log that the run is finished\n",
    "    print('\\n\\nFinished.')\n",
    "    print('\\nEnd time is: ', datetime.datetime.today())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    userlist = ['s002','s032','s036','s047','s052']\n",
    "    start_line = 0\n",
    "    end_line = 1\n",
    "    decay_rate = 0.1\n",
    "    \n",
    "    \n",
    "    \n",
    "    for _,user in enumerate (userlist):\n",
    "        out_put_filename = 'C01'+user+'_'+str(start_line)+'_'+str(end_line)+'decay'+str(decay_rate)+'.json'\n",
    "        run (user=user,start_line=start_line,end_line=end_line,exp_dir='.',output_filename=out_put_filename,decay_rate=decay_rate)      \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'event' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cc102de9059f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'event' is not defined"
     ]
    }
   ],
   "source": [
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
